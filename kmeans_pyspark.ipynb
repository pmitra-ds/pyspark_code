{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ebc7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "from numpy import array, random\n",
    "from math import sqrt\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc784a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(2023)\n",
    "\n",
    "# Generate cluster data\n",
    "def generate_cluster_data(num_samples, num_features, num_clusters, cluster_std):\n",
    "    X, y = make_blobs(\n",
    "        n_samples=num_samples,\n",
    "        n_features=num_features,\n",
    "        centers=num_clusters,\n",
    "        cluster_std=cluster_std,\n",
    "        random_state=0\n",
    "    )\n",
    "    return X, y\n",
    "\n",
    "# Parameters for cluster data generation\n",
    "num_samples = 5000  # Total number of samples\n",
    "num_features = 4  # Number of features (dimensions)\n",
    "num_clusters = 4  # Number of clusters\n",
    "cluster_std = 1.0  # Standard deviation of each cluster\n",
    "\n",
    "# Generate cluster data\n",
    "X, y = generate_cluster_data(num_samples, num_features, num_clusters, cluster_std)\n",
    "\n",
    "# Plot the generated data\n",
    "plt.scatter(X[:, 0], X[:,1], c=y, cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Clustered Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d1f45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "K = 5\n",
    "\n",
    "# Set up Spark configuration and context\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"SparkKMeans\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = sc.parallelize(X)\n",
    "\n",
    "\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(data, K, maxIterations=10,\n",
    "           initializationMode=\"random\")\n",
    "\n",
    "# Print out the cluster assignments\n",
    "resultRDD = data.map(lambda point: clusters.predict(point)).cache()\n",
    "\n",
    "print(\"Counts by value:\")\n",
    "counts = resultRDD.countByValue()\n",
    "#print(counts)\n",
    "\n",
    "\n",
    "print(\"Cluster assignments:\")\n",
    "results = resultRDD.collect()\n",
    "#print(results)\n",
    "\n",
    "# Calculate Within Set Sum of Squared Error (WSSSE)\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = data.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c381fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
